{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFuK3D_9PvHr"
      },
      "source": [
        "# Exploring Cerebras Model Variants\n",
        "\n",
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSkez75fZoo82SccEXRMVRlj9sZsQifRUhURQ&s\" width=\"200\">\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Gs5LOcyhLkoXj1ReYQVhH5rTD21FEr4S?usp=sharing)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Cerebras offers several powerful model variants through its ultra-fast inference platform, each optimized for different use cases and performance characteristics. This notebook explores the features of each Cerebras model variant and provides examples of how to use them effectively in your applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkRPZB-9Q4Kt"
      },
      "source": [
        "## Best Practices for Using Cerebras Models\n",
        "\n",
        "Here are some best practices for getting the most out of Cerebras model variants:\n",
        "\n",
        "1. **Select the right model for your task**:\n",
        "   - Use Llama 3.3 70B for general-purpose tasks requiring high accuracy\n",
        "   - Use Qwen 3 Coder 480B for code generation, debugging, and technical tasks\n",
        "   - Use GPT-OSS-120B for open-source alternatives with strong performance\n",
        "\n",
        "2. **Optimize your prompts**:\n",
        "   - Be specific and clear in your instructions\n",
        "   - Provide context when necessary\n",
        "   - Use examples to guide the model's output format\n",
        "\n",
        "3. **Adjust temperature settings**:\n",
        "   - Lower temperature (0-0.3) for more deterministic, focused responses\n",
        "   - Higher temperature (0.7-1.0) for more creative, varied responses\n",
        "\n",
        "4. **Consider token limits**:\n",
        "   - Be mindful of input and output token limits\n",
        "   - Break complex tasks into smaller, manageable chunks\n",
        "\n",
        "5. **Leverage Cerebras speed**:\n",
        "   - Use streaming for real-time applications\n",
        "   - Batch requests when processing multiple items"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Your API Keys\n",
        "\n",
        "Before you begin, make sure you have:\n",
        "\n",
        "1. A Cerebras API key (Get yours at [Cerebras Cloud](https://cloud.cerebras.ai/))\n",
        "2. Basic familiarity with Python and Jupyter notebooks\n",
        "\n",
        "This notebook is designed to run in Google Colab, so no local Python installation is required."
      ],
      "metadata": {
        "id": "prereqs_cell"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-JDYIwlPvHt"
      },
      "source": [
        "\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, let's install the necessary libraries and set up our environment."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai requests"
      ],
      "metadata": {
        "id": "o_-XYDDVQB2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import necessary libraries\n"
      ],
      "metadata": {
        "id": "ratVVonyQD1e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MMt03I-PvHu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "from IPython.display import display, Markdown, HTML\n",
        "from google.colab import userdata\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0eKXF6YPvHu"
      },
      "source": [
        "## Authentication\n",
        "\n",
        "To use the Cerebras API, you need to set up your API key.\n",
        "\n",
        "Set up authentication using Colab secrets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4u4UxBWIPvHv"
      },
      "outputs": [],
      "source": [
        "os.environ[\"CEREBRAS_API_KEY\"] = userdata.get('CEREBRAS_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create the OpenAI client with Cerebras"
      ],
      "metadata": {
        "id": "oPRdHAb1QPlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(\n",
        "    base_url=\"https://api.cerebras.ai/v1\",\n",
        "    api_key=os.environ.get(\"CEREBRAS_API_KEY\")\n",
        ")"
      ],
      "metadata": {
        "id": "fzIZRpcBQT4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkTwAZP4PvHv"
      },
      "source": [
        "## Helper Functions\n",
        "\n",
        "Let's create helper functions to interact with the Cerebras API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMbB6-0ZPvHv"
      },
      "outputs": [],
      "source": [
        "def get_model_response(model, prompt, temperature=0.7, max_tokens=1024):\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "def benchmark_model(model, prompt, temperature=0.7):\n",
        "    start_time = time.time()\n",
        "    response = get_model_response(model, prompt, temperature)\n",
        "    end_time = time.time()\n",
        "\n",
        "    return {\n",
        "        \"model\": model,\n",
        "        \"response\": response,\n",
        "        \"time\": end_time - start_time,\n",
        "        \"length\": len(response)\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf0h0GMSPvHv"
      },
      "source": [
        "## Llama 3.3 70B: Latest General Purpose Model\n",
        "\n",
        "Llama 3.3 70B is the latest and most advanced general-purpose model available on Cerebras, offering state-of-the-art performance across a wide range of tasks.\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- Superior performance on general tasks\n",
        "- Excellent instruction following\n",
        "- Strong reasoning capabilities\n",
        "- Good balance of speed and accuracy\n",
        "- Wide context window\n",
        "\n",
        "### Example: General Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyDjOp1JPvHv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4fdca51-df3d-466c-ea50-21a9411627f4"
      },
      "outputs": [],
      "source": [
        "general_examples = [\n",
        "    \"Explain quantum computing in simple terms.\",\n",
        "    \"Write a professional email requesting a meeting.\",\n",
        "    \"What are the key differences between machine learning and deep learning?\"\n",
        "]\n",
        "\n",
        "for i, prompt in enumerate(general_examples, 1):\n",
        "    print(f\"\\n--- Example {i} ---\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    response = get_model_response(\"llama3.3-70b\", prompt)\n",
        "    print(f\"\\nResponse:\\n{response}\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmAe7itgPvHw"
      },
      "source": [
        "### Example: Code Generation\n",
        "\n",
        "Llama 3.3 70B excels at code generation tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jXDlPMTPvHw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba1e919e-442c-4e71-8cd7-875e94b17e1b"
      },
      "outputs": [],
      "source": [
        "code_prompt = \"Write a Python function that finds the nth Fibonacci number using dynamic programming.\"\n",
        "\n",
        "print(\"Code Generation Example:\")\n",
        "print(f\"Prompt: {code_prompt}\")\n",
        "\n",
        "response = get_model_response(\"llama3.3-70b\", code_prompt, temperature=0.2)\n",
        "print(f\"\\nResponse:\\n{response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woefn02PPvHw"
      },
      "source": [
        "## Qwen 3 Coder 480B: Advanced Code Model\n",
        "\n",
        "Qwen 3 Coder 480B is a specialized model designed specifically for code generation, debugging, and technical tasks. It's one of the most powerful coding models available on Cerebras.\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- Exceptional code generation capabilities\n",
        "- Multi-language programming support\n",
        "- Code debugging and optimization\n",
        "- Technical documentation generation\n",
        "- Algorithm implementation\n",
        "\n",
        "### Example: Code Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bw9hLtdMPvHw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b76477b0-0fdb-4eed-b764-cdbc770c9cb2"
      },
      "outputs": [],
      "source": [
        "code_examples = [\n",
        "    \"Write a Python function to implement a binary search tree with insert, search, and delete operations.\",\n",
        "    \"Create a JavaScript function that debounces user input events.\",\n",
        "    \"Implement a sorting algorithm in C++ that uses the quicksort method.\"\n",
        "]\n",
        "\n",
        "for i, prompt in enumerate(code_examples, 1):\n",
        "    print(f\"\\n--- Code Example {i} ---\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    response = get_model_response(\"qwen-3-coder-480b\", prompt, temperature=0.2)\n",
        "    print(f\"\\nResponse:\\n{response}\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sHqeJ9uPvHw"
      },
      "source": [
        "### Example: Code Debugging and Optimization\n",
        "\n",
        "Qwen 3 Coder 480B excels at debugging and optimizing existing code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sM1UEnMPvHw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcbdd559-e852-428a-858d-d277f9a70c0b"
      },
      "outputs": [],
      "source": [
        "buggy_code = \"\"\"\n",
        "def fibonacci(n):\n",
        "    if n <= 1:\n",
        "        return n\n",
        "    return fibonacci(n-1) + fibonacci(n-2)\n",
        "\n",
        "# This code is inefficient for large n values\n",
        "\"\"\"\n",
        "\n",
        "debug_prompt = f\"\"\"Analyze this code and suggest optimizations:\n",
        "\n",
        "{buggy_code}\n",
        "\n",
        "Provide an optimized version using dynamic programming or memoization.\"\"\"\n",
        "\n",
        "print(\"Code Debugging and Optimization Example:\")\n",
        "response = get_model_response(\"qwen-3-coder-480b\", debug_prompt, temperature=0.2)\n",
        "print(f\"\\nResponse:\\n{response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExQPq2YjPvHx"
      },
      "source": [
        "## GPT-OSS-120B: Open Source Alternative\n",
        "\n",
        "GPT-OSS-120B is a powerful open-source model that provides strong performance across various tasks, offering a balance between capability and accessibility.\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- Open-source architecture\n",
        "- Strong general-purpose capabilities\n",
        "- Versatile for multiple use cases\n",
        "- Good reasoning abilities\n",
        "- Balanced performance\n",
        "\n",
        "### Example: General Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McfxIoOiPvHx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be22f041-6ebe-4972-ab4f-4028ff89157b"
      },
      "outputs": [],
      "source": [
        "general_prompts = [\n",
        "    \"Explain the concept of blockchain technology.\",\n",
        "    \"What are the key principles of object-oriented programming?\",\n",
        "    \"Describe the water cycle in nature.\",\n",
        "    \"What is the difference between HTTP and HTTPS?\",\n",
        "    \"Explain the concept of supply and demand in economics.\"\n",
        "]\n",
        "\n",
        "for prompt in general_prompts:\n",
        "    print(f\"\\nPrompt: {prompt}\")\n",
        "    response = get_model_response(\"gpt-oss-120b\", prompt, max_tokens=200)\n",
        "    print(f\"Response: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hgNfkHKPvHx"
      },
      "source": [
        "### Example: Creative Writing and Reasoning\n",
        "\n",
        "GPT-OSS-120B handles creative tasks and logical reasoning effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaWR8hfePvHx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd8aa231-0ac9-47b2-c52b-698e594367c6"
      },
      "outputs": [],
      "source": [
        "creative_prompts = [\n",
        "    \"Write a short poem about artificial intelligence.\",\n",
        "    \"Solve this logic puzzle: If all roses are flowers and some flowers fade quickly, what can we conclude?\",\n",
        "    \"Create a compelling product description for a smart home device.\"\n",
        "]\n",
        "\n",
        "print(\"Creative Writing and Reasoning Examples:\")\n",
        "for i, prompt in enumerate(creative_prompts, 1):\n",
        "    print(f\"\\n--- Example {i} ---\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    response = get_model_response(\"gpt-oss-120b\", prompt, temperature=0.7)\n",
        "    print(f\"Response: {response}\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAARZR-p4Dgs"
      },
      "source": [
        "## Model Comparison\n",
        "\n",
        "Let's compare the different models on the same task to see their relative performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvPFmqDi4Dgs",
        "outputId": "39efb294-0da3-4290-8713-bbfc67de1552"
      },
      "outputs": [],
      "source": [
        "comparison_prompt = \"Explain the concept of neural networks in 2-3 sentences.\"\n",
        "\n",
        "models = [\"llama3.3-70b\", \"qwen-3-coder-480b\", \"gpt-oss-120b\"]\n",
        "\n",
        "print(f\"Prompt: {comparison_prompt}\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "results = []\n",
        "for model in models:\n",
        "    result = benchmark_model(model, comparison_prompt, temperature=0.5)\n",
        "    results.append(result)\n",
        "\n",
        "    print(f\"\\nModel: {model}\")\n",
        "    print(f\"Response: {result['response']}\")\n",
        "    print(f\"Time: {result['time']:.3f}s\")\n",
        "    print(f\"Length: {result['length']} characters\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-CkJL-l4Dgs"
      },
      "source": [
        "## Performance Benchmarking\n",
        "\n",
        "Let's benchmark the models across different types of tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYDgPz-F4Dgs",
        "outputId": "b7fa127d-dafe-471a-b064-6dcf0d7448f8"
      },
      "outputs": [],
      "source": [
        "benchmark_tasks = {\n",
        "    \"Simple Q&A\": \"What is Python?\",\n",
        "    \"Code Generation\": \"Write a function to reverse a string in Python.\",\n",
        "    \"Creative Writing\": \"Write a haiku about technology.\",\n",
        "    \"Analysis\": \"What are the main causes of climate change?\"\n",
        "}\n",
        "\n",
        "print(\"Performance Benchmark Results\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "for task_name, prompt in benchmark_tasks.items():\n",
        "    print(f\"\\n{task_name}: {prompt}\")\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    for model in models:\n",
        "        result = benchmark_model(model, prompt)\n",
        "        print(f\"{model:20s} | Time: {result['time']:6.3f}s | Length: {result['length']:4d} chars\")\n",
        "\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTFXtNvl4Dgs"
      },
      "source": [
        "## Use Case Recommendations\n",
        "\n",
        "Based on the characteristics of each model, here are recommended use cases:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlrFUSJg4Dgs",
        "outputId": "9c76f558-18a0-4f2f-9a8a-2f1131883f39"
      },
      "outputs": [],
      "source": [
        "use_cases = {\n",
        "    \"llama3.3-70b\": [\n",
        "        \"General-purpose applications\",\n",
        "        \"Content creation and writing\",\n",
        "        \"Complex question answering\",\n",
        "        \"Instruction following tasks\",\n",
        "        \"Multi-domain problem solving\"\n",
        "    ],\n",
        "    \"qwen-3-coder-480b\": [\n",
        "        \"Code generation and development\",\n",
        "        \"Debugging and code optimization\",\n",
        "        \"Technical documentation\",\n",
        "        \"Algorithm implementation\",\n",
        "        \"Multi-language programming support\"\n",
        "    ],\n",
        "    \"gpt-oss-120b\": [\n",
        "        \"General knowledge queries\",\n",
        "        \"Creative writing tasks\",\n",
        "        \"Reasoning and logic problems\",\n",
        "        \"Educational content\",\n",
        "        \"Open-source projects\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "for model, cases in use_cases.items():\n",
        "    print(f\"\\n{model}:\")\n",
        "    for case in cases:\n",
        "        print(f\"  • {case}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iXLHYeiPvHx"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook has explored the features and capabilities of different Cerebras model variants. Each model is designed with specific strengths and use cases in mind:\n",
        "\n",
        "- **Llama 3.3 70B**: Best for general-purpose tasks requiring high accuracy and strong instruction following\n",
        "- **Qwen 3 Coder 480B**: Specialized for code generation, debugging, and technical programming tasks\n",
        "- **GPT-OSS-120B**: Open-source alternative with strong general capabilities and reasoning\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "1. **Speed**: All Cerebras models benefit from ultra-fast inference powered by the Wafer Scale Engine\n",
        "2. **Specialization**: Choose specialized models (like Qwen Coder) for domain-specific tasks\n",
        "3. **Versatility**: General-purpose models (Llama 3.3, GPT-OSS) handle diverse applications\n",
        "4. **Flexibility**: Each model offers unique strengths - select based on your specific needs\n",
        "5. **Performance**: Cerebras delivers exceptional speed across all model variants\n",
        "\n",
        "By understanding the unique capabilities of each model, you can select the most appropriate one for your specific use case and maximize the benefits of Cerebras's ultra-fast inference platform.\n",
        "\n",
        "For the latest information on Cerebras models and their capabilities, visit the [official documentation](https://inference-docs.cerebras.ai/)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
