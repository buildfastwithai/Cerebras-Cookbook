{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea1OXT-EBDCG"
      },
      "source": [
        "<div style=\"display: flex; align-items: center; gap: 40px;\">\n",
        "\n",
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSkez75fZoo82SccEXRMVRlj9sZsQifRUhURQ&s\" width=\"300\">\n",
        "\n",
        "<div>\n",
        "  <h2>Cerebras Inference</h2>\n",
        "  <p>Cerebras Systems builds the world's largest computer chip - the Wafer Scale Engine (WSE) - designed specifically for AI workloads. This cookbook provides comprehensive examples, tutorials, and best practices for developing and deploying AI models using Cerebras infrastructure, including both training on WSE clusters and fast inference via Cerebras Cloud.</p>\n",
        "</div>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/17BMZjby9ZZ6cJopyu4xWiAE6Nj3gNIBy?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07OXmu0_-mRg"
      },
      "source": [
        "# üß† Getting Started with Cerebras SDK\n",
        "\n",
        "This notebook provides a comprehensive guide to using the Cerebras SDK for AI inference. We'll cover:\n",
        "\n",
        "1. **Setup and Installation** - Installing the SDK and configuring API keys\n",
        "2. **Basic Chat Completions** - Simple text generation examples\n",
        "3. **Advanced Usage** - Streaming, function calling Examples\n",
        "4. **Model Comparison** - Testing different models including gpt-oss-120b\n",
        "\n",
        "## üìã Prerequisites\n",
        "\n",
        "- A Cerebras account and API key from [Cerebras Cloud](https://cloud.cerebras.ai/)\n",
        "- Python 3.7+\n",
        "- Basic understanding of Python and AI/ML concepts\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDRypY3u-mRi"
      },
      "source": [
        "## Step 1: Installation and Setup\n",
        "\n",
        "First, let's install the required packages and set up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9tmjRMl-mRi"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install --upgrade cerebras_cloud_sdk langchain langchain-community python-dotenv\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from typing import List, Dict, Any\n",
        "from cerebras.cloud.sdk import Cerebras\n",
        "from google.colab import userdata\n",
        "\n",
        "print(\"‚úÖ Installation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALsasTr6-mRj"
      },
      "source": [
        "## Step 2: API Key Configuration\n",
        "\n",
        "Set up your Cerebras API key. **Never hardcode your API key in production code!**\n",
        "\n",
        "Get your API key from: https://cloud.cerebras.ai/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QQPVIzg-mRj",
        "outputId": "bd14fbbf-22c7-4664-ea05-1b77b4a429e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ Cerebras client initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "# Initialize the Cerebras client\n",
        "client = Cerebras(\n",
        "    api_key=userdata.get(\"CEREBRAS_API_KEY\"),\n",
        ")\n",
        "\n",
        "print(\"üéØ Cerebras client initialized successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N54Go2b1B21e"
      },
      "source": [
        "### Example 1: Basic Chat Completion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JeQ8kHaA0f2"
      },
      "outputs": [],
      "source": [
        "print(\"üìù EXAMPLE 1: Basic Chat Completion\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create a simple chat completion\n",
        "response = client.chat.completions.create(\n",
        "      model=\"gpt-oss-120b\",  # Using Llama 3.1 8B model\n",
        "      messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are a helpful AI assistant specialized in explaining complex topics clearly.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": \"Explain what makes Cerebras Wafer-Scale Engine unique for AI training in simple terms.\"\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=500,\n",
        "            temperature=0.7,\n",
        "        )\n",
        "\n",
        "print(\"ü§ñ Model Response:\")\n",
        "print(\"-\" * 40)\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MlR-LnfB6tp"
      },
      "source": [
        "### Example 2: Complex Reasoning Task Using GPT-OSS-120B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkD5pIveBZ_I"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéØ EXAMPLE 2: GPT-OSS-120B Model Usage\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "# Complex reasoning task using GPT-OSS-120B\n",
        "response = client.chat.completions.create(\n",
        "      model=\"gpt-oss-120b\",  # Using the powerful GPT-OSS-120B model\n",
        "      messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are an expert software architect and AI researcher. Provide detailed, technical insights.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": \"\"\"\n",
        "                    Design a scalable architecture for a real-time AI inference system that can handle:\n",
        "                    1. 10,000+ concurrent requests\n",
        "                    2. Multiple model types (text, image, multimodal)\n",
        "                    3. Sub-100ms latency requirements\n",
        "                    4. Global deployment across 5 regions\n",
        "\n",
        "                    Include specific technologies, patterns, and considerations.\n",
        "                    \"\"\"\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=1000,\n",
        "            temperature=0.3,  # Lower temperature for more focused technical responses\n",
        "            top_p=0.9,\n",
        "        )\n",
        "\n",
        "print(\"üß† GPT-OSS-120B Response:\")\n",
        "print(\"-\" * 50)\n",
        "print(response.choices[0].message.content)\n",
        "print(\"-\" * 50)\n",
        "print(f\"Model: {response.model}\")\n",
        "print(f\"Tokens used: {response.usage.total_tokens}\")\n",
        "print(f\"Completion tokens: {response.usage.completion_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMf2E3ZuB-tK"
      },
      "source": [
        "### Example 3: Streaming Responses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cu1RYuluB_ke"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üåä EXAMPLE 3: Streaming Responses\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"üîÑ Starting streaming response...\")\n",
        "print(\"üìù Response (streaming):\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Create streaming completion\n",
        "stream = client.chat.completions.create(\n",
        "    model=\"gpt-oss-120b\",  # Using larger model for better responses\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Write a creative short story about an AI that discovers it's running on a Cerebras Wafer-Scale Engine. Make it engaging and technical.\"\n",
        "        }\n",
        "    ],\n",
        "    max_tokens=800,\n",
        "    temperature=0.8,\n",
        "    stream=True,  # Enable streaming\n",
        ")\n",
        "\n",
        "# Process streaming chunks\n",
        "full_response = \"\"\n",
        "for chunk in stream:\n",
        "    if chunk.choices[0].delta.content is not None:\n",
        "        content = chunk.choices[0].delta.content\n",
        "        print(content, end=\"\", flush=True)\n",
        "        full_response += content\n",
        "\n",
        "print(\"\\n\" + \"-\" * 40)\n",
        "print(f\"üìä Total characters streamed: {len(full_response)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QqKFo7K-rrz"
      },
      "source": [
        "### Example 4: Advanced Parameters And Function Calling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvJ9ySHpC82j"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üå°Ô∏è EXAMPLE 4: Testing Different Temperatures\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "question = \"Write a creative story about AI\"\n",
        "\n",
        "# Low temperature = more focused\n",
        "print(\"\\n‚Ä¢ Low temperature (0.2) - Focused response:\")\n",
        "response_low = client.chat.completions.create(\n",
        "    model=\"gpt-oss-120b\", # Using a model that supports temperature\n",
        "    messages=[{\"role\": \"user\", \"content\": question}],\n",
        "    max_tokens=200,\n",
        "    temperature=0.2  # Low = more predictable\n",
        ")\n",
        "print(response_low.choices[0].message.content)\n",
        "\n",
        "# High temperature = more creative\n",
        "print(\"\\n‚Ä¢ High temperature (0.9) - Creative response:\")\n",
        "response_high = client.chat.completions.create(\n",
        "    model=\"gpt-oss-120b\", # Using a model that supports temperature\n",
        "    messages=[{\"role\": \"user\", \"content\": question}],\n",
        "    max_tokens=200,\n",
        "    temperature=0.9  # High = more creative\n",
        ")\n",
        "print(response_high.choices[0].message.content)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3C47sul_Nz7"
      },
      "source": [
        "### Basic Function Calling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBsuuycN-7gV"
      },
      "outputs": [],
      "source": [
        "print(\"üõ†Ô∏è Function Calling\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Simple function definition\n",
        "simple_tool = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"add_numbers\",\n",
        "            \"description\": \"Add two numbers together\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"number1\": {\"type\": \"number\", \"description\": \"First number\"},\n",
        "                    \"number2\": {\"type\": \"number\", \"description\": \"Second number\"}\n",
        "                },\n",
        "                \"required\": [\"number1\", \"number2\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "try:\n",
        "    print(\"ü§ñ Asking AI to use the add_numbers function:\")\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-oss-120b\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": \"Please add 25 and 17 using the add_numbers function\"}\n",
        "        ],\n",
        "        tools=simple_tool,\n",
        "        tool_choice=\"auto\",  # Let AI decide when to use tools\n",
        "        max_tokens=200,\n",
        "        temperature=0.3\n",
        "    )\n",
        "\n",
        "    # Check if AI wants to call our function\n",
        "    if response.choices[0].message.tool_calls:\n",
        "        print(\"‚úÖ AI decided to use the function!\")\n",
        "\n",
        "        for tool_call in response.choices[0].message.tool_calls:\n",
        "            print(f\"Function name: {tool_call.function.name}\")\n",
        "            print(f\"Function arguments: {tool_call.function.arguments}\")\n",
        "\n",
        "            # Parse the arguments and do the calculation\n",
        "            args = json.loads(tool_call.function.arguments)\n",
        "            result = args[\"number1\"] + args[\"number2\"]\n",
        "            print(f\"üßÆ Calculation result: {args['number1']} + {args['number2']} = {result}\")\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è AI responded without using the function:\")\n",
        "        print(response.choices[0].message.content)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d086cebe"
      },
      "source": [
        "### Example 5: Code Generation with Function Calling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe570b0d"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üíª EXAMPLE 6: Code Generation with Function Calling\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Define the generate_code function tool\n",
        "generate_code_tool = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"generate_code\",\n",
        "            \"description\": \"Generate code snippets in specified language\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"language\": {\"type\": \"string\", \"description\": \"Programming language\"},\n",
        "                    \"task\": {\"type\": \"string\", \"description\": \"What the code should do\"},\n",
        "                    \"complexity\": {\"type\": \"string\", \"enum\": [\"simple\", \"medium\", \"advanced\"]}\n",
        "                },\n",
        "                \"required\": [\"language\", \"task\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "try:\n",
        "    print(\"ü§ñ Asking AI to generate code:\")\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-oss-120b\",\n",
        "        messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Generate Python code to sort a list of numbers, medium complexity\"\n",
        "        }],\n",
        "        tools=generate_code_tool,\n",
        "        tool_choice=\"auto\",\n",
        "        max_tokens=400,\n",
        "        temperature=0.3\n",
        "    )\n",
        "\n",
        "    if response.choices[0].message.tool_calls:\n",
        "        print(\"‚úÖ AI decided to use the function!\")\n",
        "        for tool_call in response.choices[0].message.tool_calls:\n",
        "            print(f\"Function: {tool_call.function.name}\")\n",
        "            args = json.loads(tool_call.function.arguments)\n",
        "            print(f\"üîß Code Request: {args}\")\n",
        "\n",
        "            # Simulate code generation\n",
        "            if tool_call.function.name == \"generate_code\":\n",
        "                print(f\"üìù Generated {args['language']} code for: {args['task']}\")\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è AI responded without using functions:\")\n",
        "        print(response.choices[0].message.content)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå An error occurred: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgrhUFGe3W2i"
      },
      "source": [
        "### Example 6: Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKALZbpr3HTr"
      },
      "outputs": [],
      "source": [
        "def example_6_model_comparison(client: Cerebras) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Compare different Cerebras models for the same task.\n",
        "\n",
        "    Args:\n",
        "        client: Initialized Cerebras client\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the results of the model comparison.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üìä EXAMPLE 6: Model Comparison\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Test prompt for comparison\n",
        "    test_prompt = \"Explain quantum computing in exactly 3 sentences.\"\n",
        "\n",
        "    # Models to compare\n",
        "    models = [\n",
        "        \"qwen-3-coder-480b\",\n",
        "        \"llama-3.3-70b\",\n",
        "        \"gpt-oss-120b\",\n",
        "        \"llama-4-maverick-17b-128e-instruct\"\n",
        "    ]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for model in models:\n",
        "        try:\n",
        "            print(f\"\\nüß™ Testing model: {model}\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": test_prompt\n",
        "                    }\n",
        "                ],\n",
        "                max_tokens=200,\n",
        "                temperature=0.5,\n",
        "            )\n",
        "\n",
        "            end_time = time.time()\n",
        "            response_time = end_time - start_time\n",
        "\n",
        "            results[model] = {\n",
        "                \"response\": response.choices[0].message.content,\n",
        "                \"tokens\": response.usage.total_tokens,\n",
        "                \"time\": response_time\n",
        "            }\n",
        "\n",
        "            print(f\"‚è±Ô∏è Response time: {response_time:.2f}s\")\n",
        "            print(f\"üìù Response: {response.choices[0].message.content[:100]}...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error with {model}: {str(e)}\")\n",
        "            results[model] = {\"error\": str(e)}\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2j1zPi-94AKV"
      },
      "outputs": [],
      "source": [
        "# Summary comparison\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"üìà COMPARISON SUMMARY\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "results = example_6_model_comparison(client)\n",
        "\n",
        "for model, result in results.items():\n",
        "    if \"error\" not in result:\n",
        "      print(f\"\\nü§ñ {model}:\")\n",
        "      print(f\"   ‚è±Ô∏è Time: {result['time']:.2f}s\")\n",
        "      print(f\"   üéØ Tokens: {result['tokens']}\")\n",
        "      print(f\"   üìè Length: {len(result['response'])} chars\")\n",
        "    else:\n",
        "      print(f\"\\nü§ñ {model}: Error - {result['error']}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
