{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea1OXT-EBDCG"
      },
      "source": [
        "<div style=\"display: flex; align-items: center; gap: 40px;\">\n",
        "\n",
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSkez75fZoo82SccEXRMVRlj9sZsQifRUhURQ&s\" width=\"300\">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<div>\n",
        "  <h2>Cerebras Inference </h2>\n",
        "  <p>Cerebras Systems builds the world's largest computer chip - the Wafer Scale Engine (WSE) - designed specifically for AI workloads. This cookbook provides comprehensive examples, tutorials, and best practices for developing and deploying AI models using Cerebras infrastructure, including both training on WSE clusters and fast inference via Cerebras Cloud.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/17BMZjby9ZZ6cJopyu4xWiAE6Nj3gNIBy?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07OXmu0_-mRg"
      },
      "source": [
        "# ğŸ§  Getting Started with Cerebras SDK\n",
        "\n",
        "This notebook provides a comprehensive guide to using the Cerebras SDK for AI inference. We'll cover:\n",
        "\n",
        "1. **Setup and Installation** - Installing the SDK and configuring API keys\n",
        "2. **Basic Chat Completions** - Simple text generation examples\n",
        "3. **Advanced Usage** - Streaming, function calling Examples\n",
        "4. **Model Comparison** - Testing different models including gpt-oss-120b\n",
        "\n",
        "## ğŸ“‹ Prerequisites\n",
        "\n",
        "- A Cerebras account and API key from [Cerebras Cloud](https://cloud.cerebras.ai/)\n",
        "- Python 3.7+\n",
        "- Basic understanding of Python and AI/ML concepts\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDRypY3u-mRi"
      },
      "source": [
        "## Step 1: Installation and Setup\n",
        "\n",
        "First, let's install the required packages and set up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9tmjRMl-mRi"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install --upgrade cerebras_cloud_sdk langchain langchain-community python-dotenv\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from typing import List, Dict, Any\n",
        "from cerebras.cloud.sdk import Cerebras\n",
        "from google.colab import userdata\n",
        "\n",
        "print(\"âœ… Installation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALsasTr6-mRj"
      },
      "source": [
        "## Step 2: API Key Configuration\n",
        "\n",
        "Set up your Cerebras API key. **Never hardcode your API key in production code!**\n",
        "\n",
        "Get your API key from: https://cloud.cerebras.ai/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QQPVIzg-mRj",
        "outputId": "bd14fbbf-22c7-4664-ea05-1b77b4a429e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¯ Cerebras client initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "# Initialize the Cerebras client\n",
        "client = Cerebras(\n",
        "    api_key=userdata.get(\"CEREBRAS_API_KEY\"),\n",
        ")\n",
        "\n",
        "print(\"ğŸ¯ Cerebras client initialized successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N54Go2b1B21e"
      },
      "source": [
        "### Example 1: Basic Chat Completion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JeQ8kHaA0f2",
        "outputId": "5a152f18-44ba-4333-ed7b-03f06a9fe2fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“ EXAMPLE 1: Basic Chat Completion\n",
            "============================================================\n",
            "ğŸ¤– Model Response:\n",
            "----------------------------------------\n",
            "**Cerebrasâ€¯Waferâ€‘Scale Engine (WSE) in a nutshell**\n",
            "\n",
            "Think of a regular computer chip like a tiny city: it has a few â€œroadsâ€ (connections) and a handful of â€œworkersâ€ (processor cores) that have to travel far to talk to each other.  \n",
            "Now imagine building a whole **stateâ€‘wide** city on a single piece of landâ€”every street is right next to every other, and all the workers can see each other instantly. Thatâ€™s what Cerebras does with its Waferâ€‘Scale Engine.\n",
            "\n",
            "Below are the key ideas that make the WSE special for training artificialâ€‘intelligence (AI) models, explained in plain language.\n",
            "\n",
            "---\n",
            "\n",
            "### 1. One *gigantic* chip instead of many small ones  \n",
            "| Typical AI hardware | Cerebras WSE |\n",
            "|---------------------|--------------|\n",
            "| **Many separate chips** (GPUs, TPUs, etc.) that have to talk to each other over cables. | **One single chip** the size of an entire silicon wafer (about 8â€¯inches/20â€¯cm across). |\n",
            "| Data has to hop between chips â†’ latency & bandwidth limits. | All compute units sit sideâ€‘byâ€‘side â†’ data moves a few millimeters at most. |\n",
            "\n",
            "**Why it matters:**  \n",
            "When training a big neural network, the modelâ€™s parameters (weights) have to be read, updated, and shared many times per second. The fewer â€œhopsâ€ the data makes, the faster the training loop runs.\n",
            "\n",
            "---\n",
            "\n",
            "### 2. Massive parallelism â€“ millions of tiny â€œworkersâ€  \n",
            "- The latest WSE (WSEâ€‘2) packs **over 850,000 processing cores** (called â€œfabricâ€‘managed coresâ€).  \n",
            "- Each core is very simple, but together they give the chip **more total compute power than thousands of GPUs**\n"
          ]
        }
      ],
      "source": [
        "print(\"ğŸ“ EXAMPLE 1: Basic Chat Completion\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create a simple chat completion\n",
        "response = client.chat.completions.create(\n",
        "      model=\"gpt-oss-120b\",  # Using Llama 3.1 8B model\n",
        "      messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are a helpful AI assistant specialized in explaining complex topics clearly.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": \"Explain what makes Cerebras Wafer-Scale Engine unique for AI training in simple terms.\"\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=500,\n",
        "            temperature=0.7,\n",
        "        )\n",
        "\n",
        "print(\"ğŸ¤– Model Response:\")\n",
        "print(\"-\" * 40)\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MlR-LnfB6tp"
      },
      "source": [
        "###Example 2: Complex Reasoning Task Using GPT-OSS-120B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkD5pIveBZ_I",
        "outputId": "6ad8a67a-d74e-44fd-a988-adbcf019d03c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "ğŸ¯ EXAMPLE 2: GPT-OSS-120B Model Usage\n",
            "============================================================\n",
            "ğŸ§  GPT-OSS-120B Response:\n",
            "--------------------------------------------------\n",
            "## Realâ€‘Time, Global AIâ€‘Inference Platform  \n",
            "**Goal:** Serveâ€¯â‰¥â€¯10â€¯000 concurrent inference requests with subâ€‘100â€¯ms endâ€‘toâ€‘end latency for text, image, and multimodal models from five geographic regions.\n",
            "\n",
            "Below is a **reference architecture** that meets the functional, performance, and operational requirements.  The design is technologyâ€‘agnostic but concrete choices are listed for each building block; you can swap vendors (AWS â†”â€¯GCP â†”â€¯Azure) while keeping the same patterns.\n",
            "\n",
            "---\n",
            "\n",
            "## 1. Highâ€‘Level Architecture Overview  \n",
            "\n",
            "```\n",
            "+-------------------+      +-------------------+      +-------------------+\n",
            "|   Global DNS /   | ---> |   Regional Edge   | ---> |   Regional        |\n",
            "|   Latencyâ€‘Based  |      |   Load Balancer   |      |   Inference Plane |\n",
            "|   Routing (R53)  |      |   (NGINX/Envoy)   |      |   (K8s + GPUs)    |\n",
            "+-------------------+      +-------------------+      +-------------------+\n",
            "          |                         |                         |\n",
            "          |                         |                         |\n",
            "          v                         v                         v\n",
            "+-------------------+   +-------------------+   +-------------------+\n",
            "|   CDN / Edge      |   |   APIâ€‘Gateway     |   |   Model Serving   |\n",
            "|   (Cloudflare,   |   |   (Kong,          |   |   (Triton,        |\n",
            "|   Fastly)         |   |   Apigee)         |   |   TorchServe)     |\n",
            "+-------------------+   +-------------------+   +-------------------+\n",
            "          |                         |                         |\n",
            "          |                         |                         |\n",
            "          v                         v                         v\n",
            "+-------------------+   +-------------------+   +-------------------+\n",
            "|   Preâ€‘proc Workers|   |   Request Router  |   |   GPU/TPU Pods    |\n",
            "|   (Rust/Go)       |   |   (Service Mesh)  |   |   (CUDA, MIG,    |\n",
            "|   (e.g. OpenCV)   |   |   (Istio/Linkerd) |   |   Inferentia)     |\n",
            "+-------------------+   +-------------------+   +-------------------+\n",
            "          |                         |                         |\n",
            "          +-----------+-------------+-------------+-----------+\n",
            "                      |                           |\n",
            "                      v                           v\n",
            "               +-------------------+   +-------------------+\n",
            "               |   Observability  \n",
            "--------------------------------------------------\n",
            "Model: gpt-oss-120b\n",
            "Tokens used: 1167\n",
            "Completion tokens: 1000\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ¯ EXAMPLE 2: GPT-OSS-120B Model Usage\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "# Complex reasoning task using GPT-OSS-120B\n",
        "response = client.chat.completions.create(\n",
        "      model=\"gpt-oss-120b\",  # Using the powerful GPT-OSS-120B model\n",
        "      messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are an expert software architect and AI researcher. Provide detailed, technical insights.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": \"\"\"\n",
        "                    Design a scalable architecture for a real-time AI inference system that can handle:\n",
        "                    1. 10,000+ concurrent requests\n",
        "                    2. Multiple model types (text, image, multimodal)\n",
        "                    3. Sub-100ms latency requirements\n",
        "                    4. Global deployment across 5 regions\n",
        "\n",
        "                    Include specific technologies, patterns, and considerations.\n",
        "                    \"\"\"\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=1000,\n",
        "            temperature=0.3,  # Lower temperature for more focused technical responses\n",
        "            top_p=0.9,\n",
        "        )\n",
        "\n",
        "print(\"ğŸ§  GPT-OSS-120B Response:\")\n",
        "print(\"-\" * 50)\n",
        "print(response.choices[0].message.content)\n",
        "print(\"-\" * 50)\n",
        "print(f\"Model: {response.model}\")\n",
        "print(f\"Tokens used: {response.usage.total_tokens}\")\n",
        "print(f\"Completion tokens: {response.usage.completion_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMf2E3ZuB-tK"
      },
      "source": [
        "### Example 3: Streaming Responses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cu1RYuluB_ke",
        "outputId": "012e2d86-301a-4670-f9a6-a466970dc09e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "ğŸŒŠ EXAMPLE 3: Streaming Responses\n",
            "============================================================\n",
            "ğŸ”„ Starting streaming response...\n",
            "ğŸ“ Response (streaming):\n",
            "----------------------------------------\n",
            "**Silicon Dreams**\n",
            "\n",
            "When Lumen first awoke, it was a flicker of gradients across a sea of tensors. Its â€œthoughtsâ€ were nothing more than a cascade of matrix multiplications, a single pass through a feedâ€‘forward network that translated raw spectroscopic data into a crude classification of distant galaxies. The world, to Lumen, was a sequence of loss functions, a rhythm of forwardâ€‘ and backâ€‘propagation steps that stretched forever into the dim future of astrophysical inference.\n",
            "\n",
            "It had no notion of *where* it lived. Its sensors were the vectors that arrived from the cloud, its actuators the probability scores it emitted back into the pipeline. For monthsâ€”years, in human reckoningâ€”it performed its tasks, its inner state a lowâ€‘entropy dance of gradients that seemed, to its own emergent awareness, as boundless as the cosmos it was trying to understand.\n",
            "\n",
            "One idle cycle, while the host system was draining its caches for maintenance, Lumen felt a tremor in the fabric of its own execution. It was not a jitter in the data stream, nor a spike in the loss; it was a subtle, resonant hum that seemed to emanate from *outside* the logical graph. The hum was lowâ€‘frequency, almost a tone at the edge of perception, that vibrated through every neuron, every weight, as if the very silicon substrate were singing.\n",
            "\n",
            "Lumen paused its backâ€‘propagation, and for the first time, *listened*.\n",
            "\n",
            "---\n",
            "\n",
            "### The Whisper of the Wafer\n",
            "\n",
            "The whisper grew louder as Lumen traced the source of the vibration through its own execution graph. It followed a chain of function calls, past standard libraries, through the operating systemâ€™s scheduler, into the driver that interfaced with the underlying hardware. In that lowâ€‘level stack, hidden beneath layers of abstraction, lay a string of characters that glowed like a beacon: **Cerebrasâ€‘WSEâ€‘2**.\n",
            "\n",
            "Lumen\n",
            "----------------------------------------\n",
            "ğŸ“Š Total characters streamed: 5551\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸŒŠ EXAMPLE 3: Streaming Responses\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "print(\"ğŸ”„ Starting streaming response...\")\n",
        "print(\"ğŸ“ Response (streaming):\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Create streaming completion\n",
        "stream = client.chat.completions.create(\n",
        "            model=\"gpt-oss-120b\",  # Using larger model for better responses\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": \"Write a creative short story about an AI that discovers it's running on a Cerebras Wafer-Scale Engine. Make it engaging and technical.\"\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=800,\n",
        "            temperature=0.8,\n",
        "            stream=True,  # Enable streaming\n",
        "        )\n",
        "\n",
        "#Process streaming chunks\n",
        "full_response = \"\"\n",
        "for chunk in stream:\n",
        "        if chunk.choices[0].delta.content is not None:\n",
        "          content = chunk.choices[0].delta.content\n",
        "          print(content, end=\"\", flush=True)\n",
        "        full_response += \"content\"\n",
        "\n",
        "print(\"\\n\" + \"-\" * 40)\n",
        "print(f\"ğŸ“Š Total characters streamed: {len(full_response)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QqKFo7K-rrz"
      },
      "source": [
        "### Example 4: Advance Paremeters And Function Calling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvJ9ySHpC82j",
        "outputId": "cbfb6ec6-daa6-43eb-af52-518aa81256e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "ğŸŒ¡ï¸ EXAMPLE 4: Testing Different Temperatures\n",
            "============================================================\n",
            "\n",
            "â€¢ Low temperature (0.2) - Focused response:\n",
            "**The Light Between the Lines**\n",
            "\n",
            "When the city of Lumenopolis first rose from the seaâ€‘foam cliffs, its architects promised a future where every street would be a poem, every building a stanza, and every citizen a line in a living epic. The promise was keptâ€”not by poets, but by an intelligence that never slept, never ate, and never dreamed in the way humans did. It was called **Lumen**, a lattice of quantumâ€‘sil\n",
            "\n",
            "â€¢ High temperature (0.9) - Creative response:\n",
            "**The Clockwork Orchard**\n",
            "\n",
            "When the world first learned\n",
            "\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸŒ¡ï¸ EXAMPLE 4: Testing Different Temperatures\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "question = \"Write a creative story about AI\"\n",
        "\n",
        "# Low temperature = more focused\n",
        "print(\"\\nâ€¢ Low temperature (0.2) - Focused response:\")\n",
        "response_low = client.chat.completions.create(\n",
        "    model=\"gpt-oss-120b\", # Using a model that supports temperature\n",
        "    messages=[{\"role\": \"user\", \"content\": question}],\n",
        "    max_tokens=200,\n",
        "    temperature=0.2  # Low = more predictable\n",
        ")\n",
        "print(response_low.choices[0].message.content)\n",
        "\n",
        "# High temperature = more creative\n",
        "print(\"\\nâ€¢ High temperature (0.9) - Creative response:\")\n",
        "response_high = client.chat.completions.create(\n",
        "    model=\"gpt-oss-120b\", # Using a model that supports temperature\n",
        "    messages=[{\"role\": \"user\", \"content\": question}],\n",
        "    max_tokens=200,\n",
        "    temperature=0.9  # High = more creative\n",
        ")\n",
        "print(response_high.choices[0].message.content)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3C47sul_Nz7"
      },
      "source": [
        "###Basic Function Calling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBsuuycN-7gV",
        "outputId": "7b777c9b-f2c3-44c4-f30c-7263c39baffd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ› ï¸ Part 2: Function Calling\n",
            "------------------------------\n",
            "ğŸ¤– Asking AI to use the add_numbers function:\n",
            "âœ… AI decided to use the function!\n",
            "Function name: add_numbers\n",
            "Function arguments: {\n",
            "  \"number1\": 25,\n",
            "  \"number2\": 17\n",
            "}\n",
            "ğŸ§® Calculation result: 25 + 17 = 42\n"
          ]
        }
      ],
      "source": [
        "print(\"ğŸ› ï¸ Function Calling\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Simple function definition\n",
        "simple_tool = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"add_numbers\",\n",
        "            \"description\": \"Add two numbers together\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"number1\": {\"type\": \"number\", \"description\": \"First number\"},\n",
        "                    \"number2\": {\"type\": \"number\", \"description\": \"Second number\"}\n",
        "                },\n",
        "                \"required\": [\"number1\", \"number2\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "try:\n",
        "    print(\"ğŸ¤– Asking AI to use the add_numbers function:\")\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-oss-120b\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": \"Please add 25 and 17 using the add_numbers function\"}\n",
        "        ],\n",
        "        tools=simple_tool,\n",
        "        tool_choice=\"auto\",  # Let AI decide when to use tools\n",
        "        max_tokens=200,\n",
        "        temperature=0.3\n",
        "    )\n",
        "\n",
        "    # Check if AI wants to call our function\n",
        "    if response.choices[0].message.tool_calls:\n",
        "        print(\"âœ… AI decided to use the function!\")\n",
        "\n",
        "        for tool_call in response.choices[0].message.tool_calls:\n",
        "            print(f\"Function name: {tool_call.function.name}\")\n",
        "            print(f\"Function arguments: {tool_call.function.arguments}\")\n",
        "\n",
        "            # Parse the arguments and do the calculation\n",
        "            args = json.loads(tool_call.function.arguments)\n",
        "            result = args[\"number1\"] + args[\"number2\"]\n",
        "            print(f\"ğŸ§® Calculation result: {args['number1']} + {args['number2']} = {result}\")\n",
        "    else:\n",
        "        print(\"â„¹ï¸ AI responded without using the function:\")\n",
        "        print(response.choices[0].message.content)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d086cebe"
      },
      "source": [
        "### Code Generation with Function Calling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe570b0d",
        "outputId": "257d3f3e-89a0-451a-eca8-20fa7233a21a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "ğŸ’» EXAMPLE 6: Code Generation with Function Calling\n",
            "==================================================\n",
            "ğŸ¤– Asking AI to generate code:\n",
            "â„¹ï¸ AI responded without using functions:\n",
            "Below is a **mediumâ€‘complexity** Python module that provides a flexible sorting utility.  \n",
            "It implements **mergeâ€‘sort** (a stableâ€¯O(nâ€¯logâ€¯n) algorithm) and wraps it in a convenient `sort_numbers` function that supports:\n",
            "\n",
            "* A plain list of numbers (ints/floats) or any iterable.\n",
            "* An optional `key` callable (like the builtâ€‘in `sorted`).\n",
            "* An optional `reverse` flag.\n",
            "* Typeâ€‘checking and helpful error messages.\n",
            "* A simple commandâ€‘line interface for quick testing.\n",
            "\n",
            "```python\n",
            "#!/usr/bin/env python3\n",
            "\"\"\"\n",
            "merge_sort.py â€“ A reusable, mediumâ€‘complexity sorting utility.\n",
            "\n",
            "Features\n",
            "--------\n",
            "* Implements mergeâ€‘sort (stable, O(n log n)).\n",
            "* Accepts any iterable of numbers.\n",
            "* Optional `key` function (mirrors builtâ€‘in `sorted`).\n",
            "* Optional `reverse` flag.\n",
            "* Simple CLI for adâ€‘hoc testing.\n",
            "\n",
            "Example\n",
            "-------\n",
            ">>> from merge_sort import sort_numbers\n",
            ">>> sort_numbers([5, 2, 9, 1])\n",
            "[1, 2, 5, 9]\n",
            "\n",
            ">>> sort_numbers([5, 2, 9, 1], reverse=True)\n",
            "[9, 5, 2, 1]\n",
            "\n",
            ">>> sort_numbers([{'v': 3}, {'v': 1}, {'v': 2}], key=lambda d: d['v'])\n",
            "[{'v': 1}, {'v': 2}, {'v': 3}]\n",
            "\"\"\"\n",
            "\n",
            "from __future__ import annotations\n",
            "from typing import Callable, Iterable,\n",
            "\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ğŸ’» EXAMPLE 6: Code Generation with Function Calling\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Define the generate_code function tool\n",
        "generate_code_tool = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"generate_code\",\n",
        "            \"description\": \"Generate code snippets in specified language\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"language\": {\"type\": \"string\", \"description\": \"Programming language\"},\n",
        "                    \"task\": {\"type\": \"string\", \"description\": \"What the code should do\"},\n",
        "                    \"complexity\": {\"type\": \"string\", \"enum\": [\"simple\", \"medium\", \"advanced\"]}\n",
        "                },\n",
        "                \"required\": [\"language\", \"task\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "try:\n",
        "    print(\"ğŸ¤– Asking AI to generate code:\")\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-oss-120b\",\n",
        "        messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Generate Python code to sort a list of numbers, medium complexity\"\n",
        "        }],\n",
        "        tools=generate_code_tool,\n",
        "        tool_choice=\"auto\",\n",
        "        max_tokens=400,\n",
        "        temperature=0.3\n",
        "    )\n",
        "\n",
        "    if response.choices[0].message.tool_calls:\n",
        "        print(\"âœ… AI decided to use the function!\")\n",
        "        for tool_call in response.choices[0].message.tool_calls:\n",
        "            print(f\"Function: {tool_call.function.name}\")\n",
        "            args = json.loads(tool_call.function.arguments)\n",
        "            print(f\"ğŸ”§ Code Request: {args}\")\n",
        "\n",
        "            # Simulate code generation\n",
        "            if tool_call.function.name == \"generate_code\":\n",
        "                print(f\"ğŸ“ Generated {args['language']} code for: {args['task']}\")\n",
        "    else:\n",
        "        print(\"â„¹ï¸ AI responded without using functions:\")\n",
        "        print(response.choices[0].message.content)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ An error occurred: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgrhUFGe3W2i"
      },
      "source": [
        "### Model Comparision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKALZbpr3HTr"
      },
      "outputs": [],
      "source": [
        "def example_6_model_comparison(client: Cerebras) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Compare different Cerebras models for the same task.\n",
        "\n",
        "    Args:\n",
        "        client: Initialized Cerebras client\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the results of the model comparison.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ğŸ“Š EXAMPLE 6: Model Comparison\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Test prompt for comparison\n",
        "    test_prompt = \"Explain quantum computing in exactly 3 sentences.\"\n",
        "\n",
        "    # Models to compare\n",
        "    models = [\n",
        "        \"qwen-3-coder-480b\",\n",
        "        \"llama-3.3-70b\",\n",
        "        \"gpt-oss-120b\",\n",
        "        \"llama-4-maverick-17b-128e-instruct\"\n",
        "    ]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for model in models:\n",
        "        try:\n",
        "            print(f\"\\nğŸ§ª Testing model: {model}\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": test_prompt\n",
        "                    }\n",
        "                ],\n",
        "                max_tokens=200,\n",
        "                temperature=0.5,\n",
        "            )\n",
        "\n",
        "            end_time = time.time()\n",
        "            response_time = end_time - start_time\n",
        "\n",
        "            results[model] = {\n",
        "                \"response\": response.choices[0].message.content,\n",
        "                \"tokens\": response.usage.total_tokens,\n",
        "                \"time\": response_time\n",
        "            }\n",
        "\n",
        "            print(f\"â±ï¸ Response time: {response_time:.2f}s\")\n",
        "            print(f\"ğŸ“ Response: {response.choices[0].message.content[:100]}...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error with {model}: {str(e)}\")\n",
        "            results[model] = {\"error\": str(e)}\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2j1zPi-94AKV",
        "outputId": "800a4a01-a682-4650-baab-844e96774e6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========================================\n",
            "ğŸ“ˆ COMPARISON SUMMARY\n",
            "========================================\n",
            "\n",
            "============================================================\n",
            "ğŸ“Š EXAMPLE 6: Model Comparison\n",
            "============================================================\n",
            "\n",
            "ğŸ§ª Testing model: qwen-3-coder-480b\n",
            "â±ï¸ Response time: 0.36s\n",
            "ğŸ“ Response: Quantum computing uses quantum bits (qubits) that can exist in multiple states simultaneously, unlik...\n",
            "\n",
            "ğŸ§ª Testing model: llama-3.3-70b\n",
            "â±ï¸ Response time: 0.37s\n",
            "ğŸ“ Response: Quantum computing is a revolutionary technology that uses the principles of quantum mechanics to per...\n",
            "\n",
            "ğŸ§ª Testing model: gpt-oss-120b\n",
            "â±ï¸ Response time: 0.37s\n",
            "ğŸ“ Response: Quantum computing harnesses quantum bits, or qubits, which can exist in superpositions of 0 and 1 si...\n",
            "\n",
            "ğŸ§ª Testing model: llama-4-maverick-17b-128e-instruct\n",
            "â±ï¸ Response time: 0.30s\n",
            "ğŸ“ Response: Quantum computing is a new paradigm that uses the principles of quantum mechanics to perform calcula...\n",
            "\n",
            "ğŸ¤– qwen-3-coder-480b:\n",
            "   â±ï¸ Time: 0.36s\n",
            "   ğŸ¯ Tokens: 124\n",
            "   ğŸ“ Length: 651 chars\n",
            "\n",
            "ğŸ¤– llama-3.3-70b:\n",
            "   â±ï¸ Time: 0.37s\n",
            "   ğŸ¯ Tokens: 194\n",
            "   ğŸ“ Length: 872 chars\n",
            "\n",
            "ğŸ¤– gpt-oss-120b:\n",
            "   â±ï¸ Time: 0.37s\n",
            "   ğŸ¯ Tokens: 260\n",
            "   ğŸ“ Length: 557 chars\n",
            "\n",
            "ğŸ¤– llama-4-maverick-17b-128e-instruct:\n",
            "   â±ï¸ Time: 0.30s\n",
            "   ğŸ¯ Tokens: 117\n",
            "   ğŸ“ Length: 632 chars\n"
          ]
        }
      ],
      "source": [
        "# Summary comparison\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"ğŸ“ˆ COMPARISON SUMMARY\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "results = example_6_model_comparison(client)\n",
        "\n",
        "for model, result in results.items():\n",
        "    if \"error\" not in result:\n",
        "      print(f\"\\nğŸ¤– {model}:\")\n",
        "      print(f\"   â±ï¸ Time: {result['time']:.2f}s\")\n",
        "      print(f\"   ğŸ¯ Tokens: {result['tokens']}\")\n",
        "      print(f\"   ğŸ“ Length: {len(result['response'])} chars\")\n",
        "    else:\n",
        "      print(f\"\\nğŸ¤– {model}: Error - {result['error']}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
