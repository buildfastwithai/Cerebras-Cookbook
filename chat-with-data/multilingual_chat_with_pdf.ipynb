{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joReEVmEQKK5"
      },
      "source": [
        "# üìö Multilingual Chat with PDF (Powered by Cerebras)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMvVqesEZ9pY"
      },
      "source": [
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSkez75fZoo82SccEXRMVRlj9sZsQifRUhURQ&s\" width=\"200\">\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/16ySkITaBNK2KIP_CZoAQEQymTLoPMXKb?usp=sharing)\n",
        "\n",
        "\n",
        "<div>\n",
        "  <h2>Cerebras Inference </h2>\n",
        "  <p>Cerebras Systems builds the world's largest computer chip - the Wafer Scale Engine (WSE) - designed specifically for AI workloads. This cookbook provides comprehensive examples, tutorials, and best practices for developing and deploying AI models using Cerebras infrastructure, including both training on WSE clusters and fast inference via Cerebras Cloud.\n",
        "</p>\n",
        "\n",
        "  </div>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGL1As_GaIt-"
      },
      "source": [
        "## Get Your API Keys\n",
        "\n",
        "Before you begin, make sure you have:\n",
        "\n",
        "1. A CEREBRAS API key (Get yours at [CEREBRAS API page](https://cloud.cerebras.ai/))\n",
        "2. Basic familiarity with Python and Jupyter notebooks\n",
        "\n",
        "This notebook is designed to run in Google Colab, so no local Python installation is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uQQHhIiN3CH"
      },
      "source": [
        "### 1. Install Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nPoxsbOLSoQ",
        "outputId": "a0287cc3-efc1-4b14-8ade-bcd6c6e19dcb"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain langchain_openai langchain-community faiss-cpu requests pypdf python-docx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6HKziXMN6-2"
      },
      "source": [
        "### STEP 2 : Setup API Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcNUI6fWLZ0i"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set the API key from Colab secrets\n",
        "os.environ[\"CEREBRAS_API_KEY\"] = userdata.get(\"CEREBRAS_API_KEY\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\") # For emebedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJqLPi81OBVR"
      },
      "source": [
        "### STEP 3 :  Load the PDF Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpZ58RjuLfeC",
        "outputId": "13f1afdd-1ca1-4286-b74e-c4ae0dc70a5f"
      },
      "outputs": [],
      "source": [
        "# üìç STEP 3: Load the PDF Document (replace 'example.pdf' as needed)\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"/content/NIPS-2017-attention-is-all-you-need-Paper.pdf\")\n",
        "documents = loader.load()\n",
        "print(f\"Loaded {len(documents)} pages.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdiY8_hpOPjp"
      },
      "source": [
        "### STEP 4 :  Split Documents into Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xRJTvRSLx6U",
        "outputId": "b819ed8f-bd3d-4ff2-c143-b1d140d75e2a"
      },
      "outputs": [],
      "source": [
        "# üìç STEP 4: Split Documents into Chunks\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100\n",
        ")\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "print(f\"Split into {len(chunks)} chunks.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGZNvievOVz4"
      },
      "source": [
        "###STEP 5 :  Create Embeddings + FAISS Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShVnNGD1L1Nf"
      },
      "outputs": [],
      "source": [
        "# üìç STEP 5: Create Embeddings + FAISS Vector Store\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGDRWRNGOaMT"
      },
      "source": [
        "###STEP 6 :  Set Up Conversation Memory and RAG Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYFEFqY4MOYy"
      },
      "outputs": [],
      "source": [
        "# üìç STEP 6: Set Up Conversation Memory and RAG Chain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "# RAG model using Sutra\n",
        "rag_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=ChatOpenAI(\n",
        "        api_key=os.getenv(\"CEREBRAS_API_KEY\"),\n",
        "        base_url=\"https://api.cerebras.ai/v1\",\n",
        "        model=\"gpt-oss-120b\",\n",
        "        temperature=0.5\n",
        "    ),\n",
        "    retriever=retriever,\n",
        "    memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PXjWo34Og9W"
      },
      "source": [
        "###STEP 7 :  Ask Questions (Supports Multiple Languages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eo8C8HTMUzI"
      },
      "outputs": [],
      "source": [
        "# üìç STEP 7: Ask Questions (Supports Multiple Languages)\n",
        "def ask_question(question, language=\"English\"):\n",
        "    rag_response = rag_chain.invoke(question)\n",
        "    context = rag_response[\"answer\"]\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are a helpful assistant that answers questions about documents.\n",
        "    Use the following context to answer the question:\n",
        "\n",
        "    CONTEXT:\n",
        "    {context}\n",
        "\n",
        "    Please respond in {language}.\n",
        "\n",
        "    Question: {question}\n",
        "    \"\"\"\n",
        "\n",
        "    chat = ChatOpenAI(\n",
        "        api_key=os.getenv(\"CEREBRAS_API_KEY\"),\n",
        "        base_url=\"https://api.cerebras.ai/v1\",\n",
        "        model=\"gpt-oss-120b\"\n",
        "    )\n",
        "\n",
        "    from langchain.schema import HumanMessage\n",
        "    response = chat.invoke([HumanMessage(content=prompt)])\n",
        "    return response.content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dm4JfHOOo79"
      },
      "source": [
        "###STEP 8: Try It Out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASF6fpBUMakG",
        "outputId": "2f631eb6-93c7-4eaf-858b-8a6f3c2e8187"
      },
      "outputs": [],
      "source": [
        "# üìç STEP 8: Try It Out!\n",
        "response = ask_question(\"what is transformer\", language=\"Hindi\")\n",
        "print(\"üîπ Response:\\n\", response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kFNYjZeOujB"
      },
      "source": [
        "##Finally Integrated UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525,
          "referenced_widgets": [
            "7b8993259afb464e8bf5a327b1cecba7",
            "1bac7c5faee94bf094f47121d14438ae",
            "62556a8e17474e0394eed801aaa2a732",
            "57724395176a42b2bb0301011781277b",
            "ca91d80ba918485eb5611012b3280251",
            "8d05bfffeaba46809a9fe09f709af742",
            "0fc52306e9344282b9e984b4f190d989",
            "a2f6ac1355d3464993994a7bcb3118d1",
            "0ad824305630404ba600dcc362779d48",
            "cfc866effa9e49deb8de02159e75742b",
            "1fda4516845f4d9a80ede49ade511ce2",
            "3f9d559a61f7437890b49e20011efab3",
            "077a8cbe28354400804324efb108ea6d",
            "c27fdf6ffb33460bb39fa26e613ef61a",
            "b10435d264c547319d0bde6128e79e3e",
            "7e0f3c44a4254974b502a4f3c4fb5893",
            "048afd0171bc409ebead748c50312031",
            "bdbe09517dae48ed8b1f1d0e75f5d49f",
            "5c4ada583feb469f968155c932bdaa6a",
            "4503a224bab246448fc4ede42f8de3a8",
            "3f579c9a4cfb43da9038352d97c18738",
            "246507511949496e8e9764bcbb2a0117",
            "14766f9b3b9e420badbb300fdc411f58",
            "79e0a2fed90241a4b0681f3e985a2989",
            "4991015f29a04889b6a8bb104f0c3882",
            "e9400da1ae4c47bd878e4936209e8c30",
            "57281cf8018841ccb5945f360dc8904d",
            "5ff86b3ca994443fbe3196e3efc18b52",
            "79feb38c93144c78b3e78ddb116d3941"
          ]
        },
        "id": "v7ENw2ZJM9iV",
        "outputId": "49d22b71-8b23-464d-caaf-812b301f981a"
      },
      "outputs": [],
      "source": [
        "# 1. Imports\n",
        "import os\n",
        "import requests\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.schema import HumanMessage\n",
        "from tempfile import NamedTemporaryFile\n",
        "\n",
        "# 2. Setup LLM (Sutra)\n",
        "def get_sutra_model():\n",
        "    return ChatOpenAI(\n",
        "        api_key=os.getenv(\"CEREBRAS_API_KEY\"),\n",
        "        base_url=\"https://api.cerebras.ai/v1\",\n",
        "        model=\"gpt-oss-120b\"\n",
        "    )\n",
        "\n",
        "# 3. Load and index PDF\n",
        "def load_and_index_pdf(pdf_path):\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    docs = loader.load()\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "    chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "    embeddings = OpenAIEmbeddings(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "    chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=get_sutra_model(),\n",
        "        retriever=vectorstore.as_retriever(),\n",
        "        memory=memory\n",
        "    )\n",
        "    return chain\n",
        "\n",
        "# 4. UI Components\n",
        "\n",
        "# PDF File Upload widget\n",
        "pdf_file_upload = widgets.FileUpload(\n",
        "    accept='.pdf',\n",
        "    multiple=False,\n",
        "    description='üìÅ Upload PDF',\n",
        "    layout=widgets.Layout(width='300px')\n",
        ")\n",
        "\n",
        "load_pdf_button = widgets.Button(\n",
        "    description=\"üîÑ Load PDF\",\n",
        "    button_style='info',\n",
        "    layout=widgets.Layout(width='150px')\n",
        ")\n",
        "\n",
        "status_output = widgets.Output()\n",
        "\n",
        "# Language dropdown\n",
        "languages = [\n",
        "    \"English\", \"Hindi\", \"Gujarati\", \"Bengali\", \"Tamil\",\n",
        "    \"Telugu\", \"Kannada\", \"Malayalam\", \"Punjabi\", \"Marathi\",\n",
        "    \"Urdu\", \"Assamese\", \"Odia\", \"Sanskrit\", \"Korean\",\n",
        "    \"Japanese\", \"Arabic\", \"French\", \"German\", \"Spanish\",\n",
        "    \"Portuguese\", \"Russian\", \"Chinese\", \"Vietnamese\", \"Thai\",\n",
        "    \"Indonesian\", \"Turkish\", \"Polish\", \"Ukrainian\", \"Dutch\",\n",
        "    \"Italian\", \"Greek\", \"Hebrew\", \"Persian\", \"Swedish\",\n",
        "    \"Norwegian\", \"Danish\", \"Finnish\", \"Czech\", \"Hungarian\",\n",
        "    \"Romanian\", \"Bulgarian\", \"Croatian\", \"Serbian\", \"Slovak\",\n",
        "    \"Slovenian\", \"Estonian\", \"Latvian\", \"Lithuanian\", \"Malay\",\n",
        "    \"Tagalog\", \"Swahili\"\n",
        "]\n",
        "\n",
        "lang_dropdown = widgets.Dropdown(\n",
        "    options=languages,\n",
        "    value=\"English\",\n",
        "    description='üåê Language:',\n",
        "    layout=widgets.Layout(width='300px')\n",
        ")\n",
        "\n",
        "chat_output = widgets.HTML(\n",
        "    value=\"<div style='padding:10px; font-family:Arial; font-size:14px; height:300px; overflow-y:auto; border:1px solid #ccc; border-radius:5px;'>Chat history will appear here...</div>\"\n",
        ")\n",
        "\n",
        "user_input = widgets.Text(\n",
        "    placeholder='Type your message...',\n",
        "    layout=widgets.Layout(flex='4', width='auto')\n",
        ")\n",
        "\n",
        "send_button = widgets.Button(\n",
        "    description=\"üì§ Send\",\n",
        "    button_style='primary',\n",
        "    layout=widgets.Layout(flex='1', width='auto')\n",
        ")\n",
        "\n",
        "messages = []\n",
        "conversation_chain = None\n",
        "\n",
        "# 5. Load PDF Logic\n",
        "def on_load_pdf(b):\n",
        "    global conversation_chain\n",
        "    uploaded_files = pdf_file_upload.value\n",
        "\n",
        "    with status_output:\n",
        "        clear_output()\n",
        "        if not uploaded_files:\n",
        "            print(\"‚ùå Please upload a PDF file first.\")\n",
        "            return\n",
        "        try:\n",
        "            print(\"‚è≥ Processing uploaded PDF...\")\n",
        "\n",
        "            # Save uploaded content to a temp file\n",
        "            uploaded_file = list(uploaded_files.values())[0]\n",
        "            with NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp:\n",
        "                tmp.write(uploaded_file['content'])\n",
        "                tmp_path = tmp.name\n",
        "\n",
        "            conversation_chain = load_and_index_pdf(tmp_path)\n",
        "            print(\"‚úÖ PDF loaded and indexed successfully!\")\n",
        "        except Exception as e:\n",
        "            print(\"‚ùå Error:\", e)\n",
        "\n",
        "load_pdf_button.on_click(on_load_pdf)\n",
        "\n",
        "# 6. Chat Interaction Logic\n",
        "def on_send_click(b):\n",
        "    global conversation_chain\n",
        "    if conversation_chain is None:\n",
        "        with status_output:\n",
        "            clear_output()\n",
        "            print(\"‚ùå Load a PDF first.\")\n",
        "        return\n",
        "\n",
        "    user_text = user_input.value.strip()\n",
        "    if not user_text:\n",
        "        return\n",
        "\n",
        "    lang = lang_dropdown.value\n",
        "    messages.append(f\"<b style='color:#13f22d;'>You:</b> {user_text}\")\n",
        "\n",
        "    context_response = conversation_chain.invoke(user_text)\n",
        "    rag_context = context_response['answer']\n",
        "\n",
        "    system_msg = f\"\"\"\n",
        "You are a helpful assistant answering based on a document.\n",
        "Use this context: {rag_context}\n",
        "Always reply in: {lang}\n",
        "Question: {user_text}\n",
        "\"\"\"\n",
        "\n",
        "    chat_model = get_sutra_model()\n",
        "    sutra_response = chat_model.invoke([HumanMessage(content=system_msg)])\n",
        "    assistant_reply = sutra_response.content.strip()\n",
        "\n",
        "    messages.append(f\"<b style='color:#007acc;'>Assistant ({lang}):</b> {assistant_reply}\")\n",
        "\n",
        "    chat_html = \"<br>\".join(messages)\n",
        "    chat_output.value = f\"<div style='padding:10px; font-family:Arial; font-size:14px; height:300px; overflow-y:auto; border:1px solid #ccc; border-radius:5px;'>{chat_html}</div>\"\n",
        "\n",
        "    user_input.value = \"\"\n",
        "\n",
        "send_button.on_click(on_send_click)\n",
        "\n",
        "# 7. Final Layout\n",
        "input_row = widgets.HBox([user_input, send_button])\n",
        "pdf_row = widgets.HBox([pdf_file_upload, load_pdf_button])\n",
        "\n",
        "ui = widgets.VBox([\n",
        "    widgets.HTML(\"<h3 style='font-family:Arial;'>üìö Multilingual Chat with PDF (Upload from Local)</h3>\"),\n",
        "    pdf_row,\n",
        "    lang_dropdown,\n",
        "    chat_output,\n",
        "    input_row,\n",
        "    status_output\n",
        "])\n",
        "\n",
        "# 8. Display the App\n",
        "display(ui)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}