{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cy2YO2HzQ5cM"
      },
      "source": [
        "<div style=\"display: flex; align-items: center; gap: 40px;\">\n",
        "\n",
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSkez75fZoo82SccEXRMVRlj9sZsQifRUhURQ&s\" width=\"200\">\n",
        "\n",
        "\n",
        "\n",
        "<div>\n",
        "  <h2>Cerebras Inference </h2>\n",
        "  <p>Cerebras Systems builds the world's largest computer chip - the Wafer Scale Engine (WSE) - designed specifically for AI workloads. This cookbook provides comprehensive examples, tutorials, and best practices for developing and deploying AI models using Cerebras infrastructure, including both training on WSE clusters and fast inference via Cerebras Cloud.\n",
        "\n",
        "<div>\n",
        "  <h2>RAG Fusion Using Cerebras</h2>\n",
        "  <p>RAG-Fusion is an enhanced version of the traditional Retrieval-Augmented Generation (RAG) model. In RAG-Fusion, after receiving a query, the model first generates related sub-queries using a large language model. These sub-queries help find more relevant documents. Instead of simply sending the retrieved documents to the model, RAG-Fusion uses a technique called Reciprocal Rank Fusion (RRF) to score and reorder the documents based on their relevance. The best-ranked documents are then used to generate a more accurate response.</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1wRGD4teTh3LyJzZwzeGCgojjGPplLbKL?usp=sharing)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YdcvY-IRTFl"
      },
      "source": [
        "## Get Your API Keys\n",
        "\n",
        "Before you begin, make sure you have:\n",
        "\n",
        "1. A CEREBRAS API key (Get yours at [CEREBRAS API page](https://cloud.cerebras.ai/))\n",
        "2. Basic familiarity with Python and Jupyter notebooks\n",
        "\n",
        "This notebook is designed to run in Google Colab, so no local Python installation is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_FcHDjKRXlK"
      },
      "source": [
        "###ðŸ”§ 1. Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "H0rhmYh2OmlK",
        "outputId": "c342db6e-f68b-4253-b6f7-b3ad5e35f503"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain_openai langchain_community chromadb langsmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pdTd9EJRc9b"
      },
      "source": [
        "###ðŸ”‘ 2. Set Environment Variables (API Keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "383zgfHiOqJv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set the API key from Colab secrets\n",
        "os.environ[\"CEREBRAS_API_KEY\"] = userdata.get(\"CEREBRAS_API_KEY\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\") # For emebedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqPzbS7Mjsqv"
      },
      "source": [
        "###ðŸ”¹ Step 3: Load Documents and Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xezANGbiQqi"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import CSVLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "loader = CSVLoader(\"/content/sample_data/california_housing_test.csv\")\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
        "documents = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w-3ClvFjvUd"
      },
      "source": [
        "###ðŸ”¹ Step 4: Setup Embeddings and Vector Store (ChromaDB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNE-zHcIiZH8"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGZ1Bvm2jx7O"
      },
      "source": [
        "###ðŸ”¹ Step 5: Create Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdlDxkP9ibuu"
      },
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxyslMauj0lH"
      },
      "source": [
        "###ðŸ”¹ Step 6: Initialize Sutra LLM and Langsmith Prompt Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_re31Hfig5N"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langsmith import Client\n",
        "import os\n",
        "\n",
        "client = Client()\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "        api_key=os.getenv(\"CEREBRAS_API_KEY\"),\n",
        "        base_url=\"https://api.cerebras.ai/v1\",\n",
        "        model=\"gpt-oss-120b\"\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV_4K4MRj3wt"
      },
      "source": [
        "###ðŸ”¹ Step 7: Load RAG-Fusion Query Generation Prompt from Langsmith"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCUPr0VLiprO"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "prompt = client.pull_prompt(\"langchain-ai/rag-fusion-query-generation\")\n",
        "\n",
        "generate_queries = (\n",
        "    prompt\n",
        "    | ChatOpenAI(temperature=0, api_key=os.getenv(\"CEREBRAS_API_KEY\"), base_url=\"https://api.cerebras.ai/v1\", model=\"gpt-oss-120b\")\n",
        "    | StrOutputParser()\n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJoTVFXsj7SP"
      },
      "source": [
        "###ðŸ”¹ Step 8: Reciprocal Rank Fusion (RRF) Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ppn9GPQ6iuxs"
      },
      "outputs": [],
      "source": [
        "from langchain.load import dumps, loads\n",
        "\n",
        "def reciprocal_rank_fusion(results: list[list], k=60):\n",
        "    fused_scores = {}\n",
        "    for docs in results:\n",
        "        # docs sorted by relevance descending\n",
        "        for rank, doc in enumerate(docs):\n",
        "            doc_str = dumps(doc)\n",
        "            if doc_str not in fused_scores:\n",
        "                fused_scores[doc_str] = 0\n",
        "            fused_scores[doc_str] += 1 / (rank + k)\n",
        "    reranked_results = [\n",
        "        (loads(doc), score)\n",
        "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    ]\n",
        "    return reranked_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhWBzHhej-Rd"
      },
      "source": [
        "###ðŸ”¹ Step 9: Build RAG-Fusion Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cq86qBpiw7W"
      },
      "outputs": [],
      "source": [
        "chain = generate_queries | retriever.map() | reciprocal_rank_fusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmDDYOGVkAu-"
      },
      "source": [
        "###ðŸ”¹ Step 10: Test the Fusion Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pk7MUjMPizBR",
        "outputId": "1a7562db-dd24-40b9-c217-2a499c20dbc5"
      },
      "outputs": [],
      "source": [
        "query = \"what are points on a mortgage\"\n",
        "results = chain.invoke(query)\n",
        "print(\"RRF Results Top 3 Documents:\")\n",
        "for doc, score in results[:3]:\n",
        "    print(f\"Score: {score:.3f} - Doc excerpt: {doc.page_content[:200]}...\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGZqPTRSkDKu"
      },
      "source": [
        "###ðŸ”¹ Step 11: Final RAG Answer Chain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5PX44fBi18l",
        "outputId": "58ad2ca2-4535-4328-c140-a42e7d89d549"
      },
      "outputs": [],
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context.\n",
        "If you don't find the answer in the context, just say that you don't know.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt_rag = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "rag_fusion_chain = (\n",
        "    {\n",
        "        \"context\": chain,\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        "    | prompt_rag\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "final_answer = rag_fusion_chain.invoke(query)\n",
        "print(\"Final RAG-Fusion Answer:\\n\", final_answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTZu1wSfkI1l"
      },
      "source": [
        "###ðŸ”¹ Step 12: Prepare Data for Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "562n0OkDjCRm"
      },
      "outputs": [],
      "source": [
        "questions = [\"How many total rooms are for housing median age of 27\"]\n",
        "responses = []\n",
        "contexts = []\n",
        "ground_truths = [\"housing median age sometimes also called median house age\"]\n",
        "\n",
        "for q in questions:\n",
        "    responses.append(rag_fusion_chain.invoke(q))\n",
        "    contexts.append([doc.page_content for doc in retriever.invoke(q)])\n",
        "\n",
        "data = {\n",
        "    \"query\": questions,\n",
        "    \"response\": responses,\n",
        "    \"context\": contexts,\n",
        "    \"ground_truth\": ground_truths\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpuLfVBzkLON"
      },
      "source": [
        "###ðŸ”¹ Step 13: Create Dataset and DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "7Xy8pgB8jFAO",
        "outputId": "ee06ab82-f9fc-432a-80b8-34064c0aa895"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "dataset = Dataset.from_dict(data)\n",
        "df = pd.DataFrame(dataset)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "MlhdpOBirlRn",
        "outputId": "ee17a887-a83d-48f8-b167-096a8dfcd0f0"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown, Latex\n",
        "\n",
        "display(Markdown(df['response'][0])) # Viewing the response"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}