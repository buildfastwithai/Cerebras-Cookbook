{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i36Uy2miiM6"
      },
      "source": [
        "<div style=\"display: flex; align-items: center; gap: 40px;\">\n",
        "\n",
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSkez75fZoo82SccEXRMVRlj9sZsQifRUhURQ&s\" width=\"240\">\n",
        "<img src=\"https://pbs.twimg.com/profile_images/1783589223406415872/3KMxGGrF_400x400.jpg\" width=\"130\">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<div>\n",
        "  <h2>Cerebras Inference</h2>\n",
        "  <p>Cerebras Systems builds the world's largest computer chip - the Wafer Scale Engine (WSE) - designed specifically for AI workloads. This cookbook provides comprehensive examples, tutorials, and best practices for developing and deploying AI models using Cerebras infrastructure, including both training on WSE clusters and fast inference via Cerebras Cloud.</p>\n",
        "\n",
        "  <h2>LiteLLM ðŸš…</h2>\n",
        "    <p>LiteLLM simplifies access to 100+ large language models (LLMs) with a unified API. It enables easy model integration, spend tracking, rate-limiting, fallbacks, and observabilityâ€”helping developers manage LLMs like OpenAI, Anthropic, Groq, Cohere, Google, and more from a single interface.</p>\n",
        "  </div>\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1KrVzIjva5AqYwuaciHBNY8eIkrnIx1VJ?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ihV9u0GjyoE"
      },
      "source": [
        "## Get Your API Keys\n",
        "\n",
        "Before you begin, make sure you have:\n",
        "\n",
        "1. A Cerebras API key (Get yours at [Cerebras Cloud](https://cloud.cerebras.ai/))\n",
        "2. Basic familiarity with Python and Jupyter notebooks\n",
        "\n",
        "This notebook is designed to run in Google Colab, so no local Python installation is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3ENRkyQj3Ql"
      },
      "source": [
        "###Cerebras using LiteLLM ðŸš…"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb1D4-IgkDJ6"
      },
      "source": [
        "###Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wr9gUotyihbT",
        "outputId": "4a3719ac-0609-40ba-d897-eaa0bd2aaf80"
      },
      "outputs": [],
      "source": [
        "!pip install -q litellm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UiR-Xy2kU0F"
      },
      "source": [
        "###Setup API Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PK09HAeAkXqn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"CEREBRAS_API_KEY\"] = userdata.get(\"CEREBRAS_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmSHlwQBkcKE"
      },
      "source": [
        "###Initialize Cerebras Model via LiteLLM ðŸš…:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN_VubTWkoJq",
        "outputId": "f8f5cb65-771b-486c-a0b7-af5d047e7b0c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import litellm\n",
        "\n",
        "api_key = os.environ.get(\"CEREBRAS_API_KEY\")\n",
        "\n",
        "response = litellm.completion(\n",
        "    model=\"cerebras/llama3.3-70b\",\n",
        "    api_key=api_key,\n",
        "    api_base=\"https://api.cerebras.ai/v1\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain how Cerebras achieves ultra-fast inference speeds.\"\n",
        "        }\n",
        "    ],\n",
        "    temperature=0.7,\n",
        "    max_tokens=500\n",
        ")\n",
        "\n",
        "print(response['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMd7W3-jljhb"
      },
      "source": [
        "###Example 1: Streaming Response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEInYJyXlp8T",
        "outputId": "1a9a059f-0091-480c-a970-3fada8e121cf"
      },
      "outputs": [],
      "source": [
        "response = litellm.completion(\n",
        "    model=\"cerebras/llama3.3-70b\",\n",
        "    api_key=api_key,\n",
        "    api_base=\"https://api.cerebras.ai/v1\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain quantum computing in simple terms\"\n",
        "        }\n",
        "    ],\n",
        "    temperature=0.8,\n",
        "    max_tokens=1024,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    if chunk.choices[0].delta.content is not None:\n",
        "        print(chunk.choices[0].delta.content, end='')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TljQpjoamM4l"
      },
      "source": [
        "### Example 2: Code Generation with Qwen Coder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nEE9QXPmrrM",
        "outputId": "5e3db983-fefe-4d6f-987d-a31dd0e4220b"
      },
      "outputs": [],
      "source": [
        "response = litellm.completion(\n",
        "    model=\"cerebras/qwen-3-coder-480b\",\n",
        "    api_key=api_key,\n",
        "    api_base=\"https://api.cerebras.ai/v1\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Write a Python function to implement a binary search algorithm with detailed comments.\"\n",
        "        }\n",
        "    ],\n",
        "    temperature=0.2,\n",
        "    max_tokens=700\n",
        ")\n",
        "\n",
        "print(response['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqSd4oBzpqAu"
      },
      "source": [
        "### Example 3: Comparing Multiple Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cl64p6qIppc8",
        "outputId": "73cdf1ac-0e2b-4ef5-ed7e-fac86b570495"
      },
      "outputs": [],
      "source": [
        "models = [\n",
        "    \"cerebras/llama3.3-70b\",\n",
        "    \"cerebras/qwen-3-coder-480b\",\n",
        "    \"cerebras/gpt-oss-120b\"\n",
        "]\n",
        "\n",
        "prompt = \"What are the key features of functional programming?\"\n",
        "\n",
        "for model in models:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Model: {model}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    response = litellm.completion(\n",
        "        model=model,\n",
        "        api_key=api_key,\n",
        "        api_base=\"https://api.cerebras.ai/v1\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.6,\n",
        "        max_tokens=300\n",
        "    )\n",
        "\n",
        "    print(response['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDLYeq9hnAWT"
      },
      "source": [
        " ### Example 4: Code Debugging with Qwen Coder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQWHP8CWnIkZ",
        "outputId": "68427693-aa06-49a2-cb23-aee7c0bb8be3"
      },
      "outputs": [],
      "source": [
        "buggy_code = '''\n",
        "def calculate_average(numbers):\n",
        "    total = 0\n",
        "    for num in numbers:\n",
        "        total += num\n",
        "    return total / len(numbers)\n",
        "\n",
        "result = calculate_average([1, 2, 3, 4, 5])\n",
        "print(result)\n",
        "'''\n",
        "\n",
        "prompt = f\"\"\"Analyze this Python code and suggest improvements for edge cases and error handling:\n",
        "\n",
        "{buggy_code}\n",
        "\n",
        "Provide an improved version with better error handling.\"\"\"\n",
        "\n",
        "response = litellm.completion(\n",
        "    model=\"cerebras/qwen-3-coder-480b\",\n",
        "    api_key=api_key,\n",
        "    api_base=\"https://api.cerebras.ai/v1\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ],\n",
        "    temperature=0.3,\n",
        "    max_tokens=600\n",
        ")\n",
        "\n",
        "print(\"Original Code:\")\n",
        "print(buggy_code)\n",
        "print(\"\\nImproved Version:\")\n",
        "print(response['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOt4xPzVKByj"
      },
      "source": [
        "### Example 5: Different Temperature Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hgetmnJKByj",
        "outputId": "aa34a825-b681-4f7d-cb49-765ffbafc79f"
      },
      "outputs": [],
      "source": [
        "temperatures = [0.1, 0.5, 0.9]\n",
        "prompt = \"Write a creative opening line for a science fiction story.\"\n",
        "\n",
        "for temp in temperatures:\n",
        "    print(f\"\\n--- Temperature: {temp} ---\")\n",
        "\n",
        "    response = litellm.completion(\n",
        "        model=\"cerebras/llama3.3-70b\",\n",
        "        api_key=api_key,\n",
        "        api_base=\"https://api.cerebras.ai/v1\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=temp,\n",
        "        max_tokens=100\n",
        "    )\n",
        "\n",
        "    print(response['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPcVjV0fKByj"
      },
      "source": [
        "### Example 6: Batch Processing Multiple Queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCTt0UEVKByj",
        "outputId": "2d967552-3f04-45d6-8186-f4379a0d1b89"
      },
      "outputs": [],
      "source": [
        "queries = [\n",
        "    \"What is machine learning?\",\n",
        "    \"Explain neural networks.\",\n",
        "    \"What is deep learning?\",\n",
        "    \"Define artificial intelligence.\",\n",
        "    \"What is natural language processing?\"\n",
        "]\n",
        "\n",
        "print(\"Batch Processing Results:\\n\")\n",
        "\n",
        "for i, query in enumerate(queries, 1):\n",
        "    print(f\"\\n{i}. Query: {query}\")\n",
        "\n",
        "    response = litellm.completion(\n",
        "        model=\"cerebras/llama3.3-70b\",\n",
        "        api_key=api_key,\n",
        "        api_base=\"https://api.cerebras.ai/v1\",\n",
        "        messages=[{\"role\": \"user\", \"content\": query}],\n",
        "        temperature=0.5,\n",
        "        max_tokens=150\n",
        "    )\n",
        "\n",
        "    print(f\"Answer: {response['choices'][0]['message']['content']}\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C4DPpLGoYVf"
      },
      "source": [
        "###Building a Simple Chatbot with LiteLLM ðŸš…"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mxWqVACoge3",
        "outputId": "3f72c299-c89c-4ff4-a047-f55a400d2506"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import litellm\n",
        "\n",
        "print(\"Cerebras Chatbot: Hello! Type 'exit' to end the conversation.\\n\")\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    if user_input.lower() == \"exit\":\n",
        "        print(\"Chatbot: Goodbye! ðŸ‘‹\")\n",
        "        break\n",
        "\n",
        "    chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    try:\n",
        "        response = litellm.completion(\n",
        "            model=\"cerebras/llama3.3-70b\",\n",
        "            api_key=api_key,\n",
        "            api_base=\"https://api.cerebras.ai/v1\",\n",
        "            messages=chat_history,\n",
        "            temperature=0.7,\n",
        "            max_tokens=500,\n",
        "        )\n",
        "\n",
        "        reply = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "        print(\"Chatbot:\", reply)\n",
        "\n",
        "        chat_history.append({\"role\": \"assistant\", \"content\": reply})\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", str(e))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlhlVcW0KByk"
      },
      "source": [
        "### Advanced: Using LiteLLM with Multiple Providers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2t6rxEnhKByk",
        "outputId": "88c108d8-fda8-4159-d327-755ce1adf0e9"
      },
      "outputs": [],
      "source": [
        "def query_with_fallback(prompt, primary_model, fallback_model):\n",
        "    try:\n",
        "        print(f\"Trying primary model: {primary_model}\")\n",
        "        response = litellm.completion(\n",
        "            model=primary_model,\n",
        "            api_key=api_key,\n",
        "            api_base=\"https://api.cerebras.ai/v1\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.7,\n",
        "            max_tokens=300\n",
        "        )\n",
        "        return response['choices'][0]['message']['content']\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Primary model failed: {e}\")\n",
        "        print(f\"Trying fallback model: {fallback_model}\")\n",
        "\n",
        "        response = litellm.completion(\n",
        "            model=fallback_model,\n",
        "            api_key=api_key,\n",
        "            api_base=\"https://api.cerebras.ai/v1\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.7,\n",
        "            max_tokens=300\n",
        "        )\n",
        "        return response['choices'][0]['message']['content']\n",
        "\n",
        "prompt = \"Explain the concept of recursion in programming.\"\n",
        "result = query_with_fallback(\n",
        "    prompt,\n",
        "    primary_model=\"cerebras/llama3.3-70b\",\n",
        "    fallback_model=\"cerebras/gpt-oss-120b\"\n",
        ")\n",
        "\n",
        "print(\"\\nResponse:\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7_JA1_4KByk"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook demonstrated how to use Cerebras models with LiteLLM, including:\n",
        "\n",
        "1. Basic completion requests\n",
        "2. Streaming responses\n",
        "3. Code generation with specialized models\n",
        "4. Model comparison\n",
        "5. Temperature settings\n",
        "6. Batch processing\n",
        "7. Building a chatbot\n",
        "8. Fallback strategies\n",
        "\n",
        "LiteLLM provides a unified interface for accessing Cerebras's ultra-fast inference platform, making it easy to integrate into your applications."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}